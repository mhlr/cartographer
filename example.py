# -*- coding: utf-8 -*-
"""demo

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/mhlr/cartographer/blob/main/notebooks/demo.ipynb
"""


# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
from builtins import *
from toolz.curried import *
import pandas as pd
import numpy as np
import janitor
from dask import dataframe as ddf
import networkx as nx
import gensim
from gensim import summarization as summ
import requests
import pylab as plt

r = requests.get('http://www.gutenberg.org/cache/epub/2009/pg2009.txt')
print(r.encoding)


doc = r.text.split('ORIGIN OF SPECIES')[3].replace('\r\n', '\n')

print(r.text)

pars = pd.Series(doc.split('\n\n')).str.replace('\n', ' ')

pars.str.len().apply(lambda x:np.log2(x+1)).astype(int).value_counts()

# Commented out IPython magic to ensure Python compatibility.
kwds = summ.keywords(doc, scores=True, lemmatize=True, words=200)
len(kwds)


from wordcloud import WordCloud
wc = WordCloud(height=1300, width=3000, background_color='white', relative_scaling=0, prefer_horizontal=.95, max_font_size=int(180))
wc.generate_from_frequencies(dict(kwds))
# wc.generate_from_text(doc) # for comparison
plt.figure(figsize=(48,14.5))
plt.imshow(wc)
plt.axis('off')

import scipy
import sklearn as sk
from sklearn import neighbors, pipeline, preprocessing, cluster

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow_hub as hub


@memoize
def tfload(model_url):
  return hub.load(model_url)

@memoize
def emb(texts, model_url):
  return tfload(model_url)(texts)

lens = pars.str.len()
nice_pars = pars[(lens >= 256) & (lens <= 1024)]
len(nice_pars), len(pars)

# Commented out IPython magic to ensure Python compatibility.
vecs = emb(tuple(nice_pars), "https://tfhub.dev/google/universal-sentence-encoder-large/5").numpy()

D = sk.metrics.pairwise_distances(vecs, metric='cosine')
R = scipy.sparse.csgraph.minimum_spanning_tree(D).max()
G = neighbors.radius_neighbors_graph(vecs, R, metric='cosine')

@curry
def clust(g, v, n):
    pipe = pipeline.Pipeline([
        ('agg', cluster.AgglomerativeClustering(n, connectivity=g, linkage='ward', affinity='euclidean'))
    ])
    labels = pipe.fit_predict(v)
    silh = sk.metrics.silhouette_samples(v, labels, metric='cosine')
    return (silh.mean(), n, labels, silh, pipe)

core = nx.k_core(nx.Graph(G))

core_pars = np.array(nice_pars)[core.nodes]
core_vecs = vecs[core.nodes]

sil_u, n, lab, sil, p = clust(nx.adjacency_matrix(core), core_vecs, 8)

len(lab), len(sil)

layers = nx.onion_layers(core)

len(core.nodes)
